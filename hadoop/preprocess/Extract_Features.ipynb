{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-1.6.0-bin-hadoop2.6/')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"tweet_classifier\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"/Users/dreyco676/nlp_spark/data/cleaned_training.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col = parts.filter(lambda l: len(l) == 2)\n",
    "training = garantee_col.map(lambda p: (p[0], p[1]))\n",
    "# Create DataFrame\n",
    "training_df = sqlContext.createDataFrame(training, [\"tweet\", \"classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(20, {8: 1.0795, 13: 1.4519, 19: 1.4843}), classification='python')\nRow(features=SparseVector(20, {1: 1.3161, 5: 1.1959, 7: 1.2182, 8: 1.0795, 9: 1.3429, 13: 1.4519, 17: 1.3048}), classification='python')\nRow(features=SparseVector(20, {0: 2.2837, 1: 1.3161, 4: 1.3906, 7: 2.4364, 8: 1.0795, 9: 1.3429, 10: 0.9202, 16: 1.364, 18: 1.2026, 19: 1.4843}), classification='python')\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(training_df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"classification\").take(3):\n",
    "    print(features_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"/Users/dreyco676/nlp_spark/data/cleaned_unclassified.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col = parts.filter(lambda l: len(l) == 2)\n",
    "unclassified = garantee_col.map(lambda p: (p[0], p[1]))\n",
    "# Create DataFrame\n",
    "unclassified_df = sqlContext.createDataFrame(training, [\"tweet\", \"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(unclassified_df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"tweet_id\").take(3):\n",
    "    print(features_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}