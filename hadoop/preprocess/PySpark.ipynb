{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/dreyco676/spark-1.6.0-bin-hadoop2.6/')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "import langid\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"tweet_classifier\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"/Users/dreyco676/nlp_spark/data/classified_tweets.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col = parts.filter(lambda l: len(l) == 2)\n",
    "training = garantee_col.map(lambda p: (p[0], p[1].strip()))\n",
    "# Create DataFrame\n",
    "training_df = sqlContext.createDataFrame(training, [\"tweet\", \"classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use langid module to classify the language to make sure we are applying the correct cleanup actions\n",
    "# https://github.com/saffsd/langid.py\n",
    "def check_lang(data_str):\n",
    "    predict_lang = langid.classify(data_str)\n",
    "    if predict_lang[1] >= .9:\n",
    "        language = predict_lang[0]\n",
    "    else:\n",
    "        language = 'NA'\n",
    "    return language\n",
    "\n",
    "check_lang_udf = udf(check_lang, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words usually refer to the most common words in a language, there is no single universal list of stop words used\n",
    "# by all natural language processing tools.\n",
    "# Reduces Dimensionality\n",
    "# removes stop words of a single Tweets (cleaned_str/row/document)\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "remove_stops_udf = udf(remove_stops, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch-all to remove other 'words' that I felt didn't add a lot of value\n",
    "# Reduces Dimensionality, gets rid of a lot of unique urls\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' '\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    \n",
    "    # break string into 'words'\n",
    "    text = data_str.split()\n",
    "    \n",
    "    # tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text)\n",
    "    for tagged_word in tagged_text:\n",
    "        if tagged_word[1] in nltk_tags:\n",
    "            cleaned_str += tagged_word[0] + ' '\n",
    "            \n",
    "    return cleaned_str\n",
    "\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets are going to use different forms of a word, such as organize, organizes, and\n",
    "# organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy,\n",
    "# democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these\n",
    "# words to return documents that contain another word in the set.\n",
    "# Reduces Dimensionality and boosts numerical measures like TFIDF\n",
    "\n",
    "# http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "# lemmatization of a single Tweets (cleaned_str/row/document)\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        lemma = lmtzr.lemmatize(word)\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "lemmatize_udf = udf(lemm_verbs, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "lang_df = training_df.withColumn(\"lang\", check_lang_udf(\"tweet\"))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df = en_df.withColumn(\"tweet\", remove_stops_udf(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"tweet\", remove_features_udf(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tweet\", tag_and_remove_udf(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"tweet\", lemmatize_udf(\"tweet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}