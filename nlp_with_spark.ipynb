{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-1.6.0-bin-hadoop2.6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import preproc as pp\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())\n",
    "numeric_label_udf = udf(pp.numeric_label, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile(\"data/raw_classified.txt\")\n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "# Create DataFrame\n",
    "data_df = sqlContext.createDataFrame(garantee_col_rdd, [\"text\", \"id\", \"text_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df = en_df.withColumn(\"stop_text\", remove_stops_udf(en_df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"feat_text\", remove_features_udf(rm_stops_df[\"stop_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df[\"feat_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing only blank spaces\n",
    "check_blanks_df = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label_df = no_blanks_df.withColumn(\"label\", numeric_label_udf(no_blanks_df['text_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "num_label_df.withColumnRenamed(num_label_df[\"lemm_text\"], \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe important since alot of the tweets only differed by url's and RT mentions\n",
    "dedup_df = num_label_df.dropDuplicates(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we care about\n",
    "data_set = dedup_df.select(num_label_df['id'], num_label_df['text'], num_label_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training & validation sets with 60% to training and use a seed value of 1987\n",
    "splits = data_set.randomSplit([0.6, 0.4])\n",
    "training_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#\n",
    "#   Spark ML Section\n",
    "#   \n",
    "#   Skip Preprocessing and use cleaned files by running next cell\n",
    "#\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already cleaned data\n",
    "def reload_checkpoint(data_rdd):\n",
    "    parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "    # Filter bad rows out\n",
    "    garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "    typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "    # Create DataFrame\n",
    "    df = sqlContext.createDataFrame(typed_rdd, [\"id\", \"text\", \"label\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load precleaned training set\n",
    "training_rdd = sc.textFile(\"data/clean_training.txt\")\n",
    "training_df = reload_checkpoint(training_rdd)\n",
    "# Load precleaned test set\n",
    "test_rdd = sc.textFile(\"data/clean_test.txt\")\n",
    "test_df = reload_checkpoint(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cvModel.transform(test_df)\n",
    "prediction_df = result.select(\"text\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n|                text|label|prediction|\n+--------------------+-----+----------+\n|big boy toy drone...|  0.0|       1.0|\n|big data result d...|  0.0|       1.0|\n|big data service ...|  0.0|       0.0|\n|       big data wild|  0.0|       0.0|\n|bigdata algorithm...|  0.0|       0.0|\n|bigdata analytics...|  0.0|       0.0|\n|cognitive technol...|  0.0|       0.0|\n|company fight rec...|  0.0|       1.0|\n|data asset inconv...|  0.0|       0.0|\n|data lover kirk b...|  0.0|       0.0|\n|data science hunc...|  0.0|       0.0|\n|datascientists ne...|  0.0|       0.0|\n|facebook data sci...|  0.0|       1.0|\n|foodindustry plan...|  0.0|       1.0|\n|future datascienc...|  0.0|       0.0|\n|gonzalezcarmen th...|  0.0|       0.0|\n|important editori...|  0.0|       0.0|\n|intel doubling co...|  0.0|       0.0|\n|iot analytics edg...|  0.0|       0.0|\n|irish start ups m...|  0.0|       1.0|\n+--------------------+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "datasci_df = prediction_df.filter(prediction_df['label']==0.0)\n",
    "datasci_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n|                text|label|prediction|\n+--------------------+-----+----------+\n|acolyte warmachin...|  1.0|       1.0|\n|alfred producthun...|  1.0|       0.0|\n|alternative thank...|  1.0|       1.0|\n|america greatest ...|  1.0|       1.0|\n|animatic hell lot...|  1.0|       1.0|\n|annual spring cle...|  1.0|       1.0|\n|anyone think secu...|  1.0|       1.0|\n|are looking someo...|  1.0|       1.0|\n|avoid fine penalt...|  1.0|       1.0|\n|bad news toe brin...|  1.0|       1.0|\n|barton armed scho...|  1.0|       1.0|\n|basic classificat...|  1.0|       1.0|\n|bedfordshire poli...|  1.0|       1.0|\n|beginning week ja...|  1.0|       1.0|\n|being new company...|  1.0|       1.0|\n|best part econ pa...|  1.0|       1.0|\n|best photo human ...|  1.0|       1.0|\n|better bulldog bu...|  1.0|       1.0|\n|better throw same...|  1.0|       1.0|\n|big idea disrupt ...|  1.0|       1.0|\n+--------------------+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "ao_df = prediction_df.filter(prediction_df['label']==1.0)\n",
    "ao_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add join back to original text\n",
    "# TODO fix raw_classification labels\n",
    "# TODO show accuracy measures"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}